{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scikit-learn Tutorial\n",
    "Author: Lauren Gliane\n",
    "\n",
    "In this tutorial, we'll go over the following topics to build our own classification model:\n",
    "1. Introduction\n",
    "2. Install\n",
    "3. Datasets\n",
    "4. Choosing a Model\n",
    "5. Training the Model\n",
    "6. Making Predictions\n",
    "7. Save/Load Models\n",
    "8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Get to know SK-learn\n",
    "### What is Scikit-learn?\n",
    "Scikit-learn (AKA sk-learn) is written in Python, an open source project, and is **one of the most used ML libraries today**. Sk-learn is built on top of Numpy, SciPy, and Matplotlib, and contains tons of algorithms ready to use to train, evaluate, and save models straight out of the box!\n",
    "\n",
    "### Why learn Scikit-learn?\n",
    "With sk-learn, we don’t need to implement complex algorithms built on a backbone of linear algebra and statistics. By using sk-learn’s ML algorithms and neural networks, we can build models faster while getting familiar with industry-standard tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "To use sk-learn, we'll need **scipy**, **numpy**, and **sklearn**. To install these, run `pip install scipy numpy scikit-learn` in your terminal.\n",
    "\n",
    "You can confirm sk-learn was installed correctly by importing something from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "When doing machine learning, we need data to train and evaluate our models, because without data, we can't learn patterns, validate performance, or generalize to unseen examples.\n",
    "\n",
    "Scikit-learn provides tools to do that via built-in datasets, dataset loading utilities, and data preprocessing functions (like [train_test_split](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com)).\n",
    "\n",
    "### Features and Labels\n",
    "In machine learning, we use data to train models to make predictions or decisions. This data is typically structured into two main parts:\n",
    "1. **Features (Input)**\n",
    "- Features are the input variables (also called independent variables) that the model uses to learn.\n",
    "- Think of them as the measurable properties or characteristics representing what causes or correlates to your output\n",
    "- Features are real numbers or are non-numeric and have been transformed into a numerical representation\n",
    "\n",
    "    **Example:** In a house price prediction dataset, features might include the number of bedrooms, square footage, and location.\n",
    "\n",
    "2. **Labels (Output)**\n",
    "- The label is the target variable (also called the dependent variable). Labels are what you want the model to predict.\n",
    "- Think of it as the correct answer for each example in the dataset.\n",
    "\n",
    "    **Example:** For house prices, the label is the actual price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 1: Pick a Dataset\n",
    "Scikit-learn comes with several built-in toy datasets that are great for learning and experimenting. These datasets are small, well-structured, and easy to load making them perfect for learning.\n",
    "\n",
    "Look through the following [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and select one that interests you for this tutorial!\n",
    "\n",
    "**Note**: Don't choose the digits dataset, which is an image dataset that requires additional preprocessing this tutorial does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import ______\n",
    "\n",
    "dataset = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Viewing your data\n",
    "Whenever you're working with new data, it's always a good idea to familiarize yourself with the feature names, your labels, and examine a bit of the data. Run the following to understand a bit about the dataset you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data \n",
    "y = dataset.target \n",
    "  \n",
    "feature_names = dataset.feature_names \n",
    "target_names = dataset.target_names \n",
    "  \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Want to use custom data?\n",
    "If we’re using an external dataset, we can use the pandas library to load and manipulate the datasets with ease. If you haven’t yet, check out our [AI Club Pandas Tutorial](https://github.com/npragin/ai-club-project-management/blob/main/tutorials/pandas-tutorial.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d624d3",
   "metadata": {},
   "source": [
    "#### A. Missing Values (imputation methods)\n",
    "Fixing missing values is very important in data preprocessing to avoid hindering your model's performance.\n",
    "\n",
    "Step 1: Identify Missing Data\n",
    "We'll use Pandas to find missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df92941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df.isnull().sum()  # Shows missing values per column\n",
    "df.info()          # Also shows counts of non-null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c2ad7",
   "metadata": {},
   "source": [
    "Step 2: Understand the Pattern\n",
    "Missing data can be:\n",
    "- MCAR (Missing Completely at Random): No pattern\n",
    "- MAR (Missing at Random): Related to other variables\n",
    "- MNAR (Missing Not at Random): Related to the missing variable itself\n",
    "\n",
    "Step 3: Select a Strategy to Handle Missing Values\n",
    "\n",
    "**A. Remove Missing Data**\n",
    "- drop rows: when a few rows are missing, and you have a large dataset\n",
    "\n",
    "`df.dropna(inplace=True)`\n",
    "\n",
    "- drop columns: if a feature has too many missing values (e.g. >50%)\n",
    "\n",
    "`df.drop(columns=['column_name'], inplace=True)`\n",
    "\n",
    "Pros: Simple method | Cons: Loss of information\n",
    "\n",
    "**B. Imputation (Fill Missing Values)**\n",
    "1. Numerical Features\n",
    "- Mean/Median Imputation (for normal/skewed distributions):\n",
    "\n",
    "`df['col'].fillna(df['col'].mean(), inplace=True)`\n",
    "- Mode Imputation (for categorical-like numerics):\n",
    "\n",
    "`df['col'].fillna(df['col'].mode()[0], inplace=True)`\n",
    "- KNN Imputer (considers similarity between samples):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "df_imputed = imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715721e5",
   "metadata": {},
   "source": [
    "#### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "| Scaler           | Use When                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------- |\n",
    "| `StandardScaler` | You want features with mean = 0 and std = 1 (default for most ML models). |\n",
    "| `MinMaxScaler`   | You need values in a fixed range, like \\[0, 1].                           |\n",
    "| `RobustScaler`   | Your data contains **outliers**.                                          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### A. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset must be split into the training set and testing set. The training set teaches our model to recognize patterns in the data while the testing set lets us check our model’s performance on never before seen data\n",
    "#### C. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset will be split into the training set and testing set.\n",
    "*Training set:* teaches our model to recognize patterns in the data\n",
    "*Testing set:* checks our model’s performance on new, never seen before data\n",
    "\n",
    "We will use the [train_test_split()](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn.model_selection module to do this.\n",
    "\n",
    "##### Deciding on a split\n",
    "80% training and 20% testing data is the most common split for larger datasets. \n",
    "Since most of sk-learn's toy datasets are small, we’ll use 60% for training and 40% for testing to ensure we can be confident about our evaluation results. \n",
    "\n",
    "To do this, the parameter responsible for train size or test size (either or both) by taking a look at the docs to see how to pass them in. By adding setting `random_state=1`, we can ensure the split is consistent with each run for reproducibility.\n",
    "\n",
    "**Result:**\n",
    "We will have four subsets of the data after splitting.\n",
    "\n",
    "- **x_train and y_train:** feature and target values for training\n",
    "\n",
    "- **x_test and y_test:** feature and target values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, random_state=1, ___) # Fill in parameter(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "| Scaler           | Use When                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------- |\n",
    "| `StandardScaler` | You want features with mean = 0 and std = 1 (default for most ML models). |\n",
    "| `MinMaxScaler`   | You need values in a fixed range, like \\[0, 1].                           |\n",
    "| `RobustScaler`   | Your data contains **outliers**.                                          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Common Steps in Data Preparation with Scikit-learn tools\n",
    "For this tutorial, we will only go over loading data, scaling, and splitting. Below shows the common steps for preprocessing and sk-learn tools used to complete them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "\n",
    "| Step                                   | What It Does                                                    | Example Tools in Scikit-learn         |\n",
    "| -------------------------------------- | --------------------------------------------------------------- | ------------------------------------- |\n",
    "| **1. Load the Data**                   | Bring data into your environment.                               | `load_iris()`, `pandas.read_csv()`    |\n",
    "| **2. Inspect & Explore**               | Understand structure, types, missing values, outliers.          | `X.shape`, `X.info()`, `X.describe()` |\n",
    "| **3. Handle Missing Values**           | Fill in or remove incomplete data.                              | `SimpleImputer`                        |\n",
    "| **4. Encode Categorical Variables**    | Convert text labels into numbers.                               | `OneHotEncoder`, `LabelEncoder`       |\n",
    "| **5. Feature Scaling**                 | Standardize or normalize values so features contribute equally. | `StandardScaler`, `MinMaxScaler`      |\n",
    "| **6. Feature Selection / Engineering** | Choose or create features that improve performance.             | `SelectKBest`, custom transformations |\n",
    "| **7. Split the Data**                  | Divide data into training and testing sets.                     | `train_test_split()`                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641397bd",
   "metadata": {},
   "source": [
    "#### Steps 1-3 Code so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 4. Choose a Model\n",
    "\n",
    "### Pick a simple model (show different ones)\n",
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "### Fit model on training data\n",
    "### hyperparameter tuning methods (gridsearch, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Make predictions\n",
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 7. Save/Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "### Accuracy scoring\n",
    "### Confusion matrix\n",
    "### Classification reporting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
