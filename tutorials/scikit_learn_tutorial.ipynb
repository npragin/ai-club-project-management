{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scikit-learn Tutorial\n",
    "Author: Lauren Gliane\n",
    "\n",
    "In this tutorial, we'll go over the following topics to build our own classification model:\n",
    "1. Introduction\n",
    "2. Install\n",
    "3. Datasets\n",
    "4. Data Preprocessing\n",
    "5. Choosing a Model\n",
    "6. Training the Model\n",
    "7. Save/Load Models\n",
    "8. Making Predictions and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Get to know SK-learn\n",
    "### What is Scikit-learn?\n",
    "Scikit-learn (AKA sk-learn) is written in Python, an open source project, and is **one of the most used ML libraries today**. Sk-learn is built on top of Numpy, SciPy, and Matplotlib, and contains tons of algorithms ready to use to train, evaluate, and save models straight out of the box!\n",
    "\n",
    "### Why learn Scikit-learn?\n",
    "With sk-learn, we don’t need to implement complex algorithms built on a backbone of linear algebra and statistics. By using sk-learn’s ML algorithms and neural networks, we can build models faster while getting familiar with industry-standard tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "To use sk-learn, we'll need **scipy**, **numpy**, and **sklearn**. To install these, run `pip install scipy numpy scikit-learn` in your terminal.\n",
    "\n",
    "You can confirm sk-learn was installed correctly by importing something from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "When doing machine learning, we need data to train and evaluate our models, because without data, we can't learn patterns, validate performance, or generalize to unseen examples.\n",
    "\n",
    "Scikit-learn provides tools to do that via built-in datasets, dataset loading utilities, and data preprocessing functions (like [train_test_split](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com)).\n",
    "\n",
    "### Features and Labels\n",
    "In machine learning, we use data to train models to make predictions or decisions. This data is typically structured into two main parts:\n",
    "1. **Features (Input)**\n",
    "- Features are the input variables (also called independent variables) that the model uses to learn.\n",
    "- Think of them as the measurable properties or characteristics representing what causes or correlates to your output\n",
    "- Features are real numbers or are non-numeric and have been transformed into a numerical representation\n",
    "\n",
    "    **Example:** In a house price prediction dataset, features might include the number of bedrooms, square footage, and location.\n",
    "\n",
    "2. **Labels (Output)**\n",
    "- The label is the target variable (also called the dependent variable). Labels are what you want the model to predict.\n",
    "- Think of it as the correct answer for each example in the dataset.\n",
    "\n",
    "    **Example:** For house prices, the label is the actual price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 1: Pick a Dataset\n",
    "Scikit-learn comes with several built-in toy datasets that are great for learning and experimenting. These datasets are small, well-structured, and easy to load making them perfect for learning.\n",
    "\n",
    "Look through the following [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and select one that interests you for this tutorial!\n",
    "\n",
    "**Note**: Don't choose the digits dataset, which is an image dataset that requires additional preprocessing this tutorial does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import ______\n",
    "\n",
    "dataset = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Viewing your data\n",
    "Whenever you're working with new data, it's always a good idea to familiarize yourself with the feature names, your labels, and examine a bit of the data. Run the following to understand a bit about the dataset you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data \n",
    "y = dataset.target \n",
    "  \n",
    "feature_names = dataset.feature_names \n",
    "target_names = dataset.target_names \n",
    "  \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Want to use custom data?\n",
    "If we’re using an external dataset, we can use the pandas library to load and manipulate the datasets with ease. If you haven’t yet, check out our [AI Club Pandas Tutorial](https://github.com/npragin/ai-club-project-management/blob/main/tutorials/pandas-tutorial.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "When working with real-world data, it often requires some preprocessing to ensure it's in the right format for training a model. This can include handling missing values, scaling features, and selecting the relevant features.\n",
    "\n",
    "The first step is always to inspect your data to understand its structure and identify any potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### A. Missing Values\n",
    "Missing values can occur for various reasons, such as data entry errors, sensor malfunctions, or respondents skipping questions in surveys. Handling missing values is crucial because they can hinder the performance of machine learning models. Sometimes, datasets will use NaN (Not a Number), null, or zero to represent missing values. It is important to read about your dataset to understand how missing values are represented.\n",
    "\n",
    "#### Identify Missing Data\n",
    "We'll use Pandas to find missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "y = pd.Series(y)\n",
    "\n",
    "print(X.isnull().sum())  # Shows number of nulls per column\n",
    "print()\n",
    "print((X == 0).sum())  # Shows number of zeros per column\n",
    "print()\n",
    "X.info()          # Also shows counts of non-null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Strategies for Handling Missing Values\n",
    "\n",
    "\n",
    "**Option 1: Remove Missing Values**\n",
    "- Drop rows (examples) with missing values, and don't predict unless all features are available\n",
    "  - Useful when a few rows are missing, and you have a large dataset\n",
    "  - `df.dropna(inplace=True)`\n",
    "\n",
    "- Drop columns (features), and don't use that feature for training or prediction\n",
    "  - Useful when a feature has many missing values and is not critical to the task\n",
    "  - `df.drop(columns=['column_name'], inplace=True)`\n",
    "\n",
    "**Option 2: Imputation (Fill Missing Values)**\n",
    "\n",
    "[SK-Learn imputers](https://scikit-learn.org/stable/api/sklearn.impute.html)\n",
    "\n",
    "- Fill missing values with a specific value, like the mean, median, or mode of the column\n",
    "  - `SimpleImputer`\n",
    "- Fit a function to the non-missing values, then use that function to fill in the missing values\n",
    "  - `IterativeImputer`\n",
    "- Use a KNN model to predict and fill in the missing values\n",
    "  - `KNNImputer`\n",
    "\n",
    "#### Step 2: Fill Missing Values (if needed)\n",
    "If you found your dataset has missing values, choose one of the strategies above to handle them. If not, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import ____\n",
    "imputer = ____\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "| Scaler           | Use When                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------- |\n",
    "| `StandardScaler` | You want features with mean = 0 and std = 1 (default for most ML models). |\n",
    "| `MinMaxScaler`   | You need values in a fixed range, like \\[0, 1].                           |\n",
    "| `RobustScaler`   | Your data contains **outliers**.                                          |\n",
    "\n",
    "**Standardization** -- `StandardScaler`\n",
    "This scales features so they have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on data and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fe5be",
   "metadata": {},
   "source": [
    "**Normalization** -- `MinMaxScaler`\n",
    "This scales features to a fixed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### C. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset will be split into the training set and testing set.\n",
    "\n",
    "- *Training set:* teaches our model to recognize patterns in the data\n",
    "- *Testing set:* checks our model’s performance on new, never seen before data\n",
    "\n",
    "We will use the [train_test_split()](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn.model_selection module to do this.\n",
    "\n",
    "##### Deciding on a split\n",
    "80% training and 20% testing data is the most common split for larger datasets. \n",
    "Since most of sk-learn's toy datasets are small, we’ll use 60% for training and 40% for testing to ensure we can be confident about our evaluation results. \n",
    "\n",
    "To do this, the parameter responsible for train size or test size (either or both) by taking a look at the docs to see how to pass them in. By adding setting `random_state=1`, we can ensure the split is consistent with each run for reproducibility.\n",
    "\n",
    "**Result:**\n",
    "We will have four subsets of the data after splitting.\n",
    "\n",
    "- **x_train and y_train:** feature and target values for training\n",
    "\n",
    "- **x_test and y_test:** feature and target values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, random_state=1, ___) # Fill in parameter(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Other Common Steps in Data Preparation with Scikit-learn tools\n",
    "For this tutorial, we will only go over loading data, scaling, and splitting. Below shows the common steps for preprocessing and sk-learn tools used to complete them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "| Step                                   | What It Does                                                    | Example Tools in Scikit-learn         |\n",
    "| -------------------------------------- | --------------------------------------------------------------- | ------------------------------------- |\n",
    "| **Encode Categorical Variables**    | Convert text labels into numbers.                               | `OneHotEncoder`, `LabelEncoder`       |\n",
    "| **Feature Selection / Engineering** | Choose the features that maximize performance.             | `SelectKBest`, `SequentialFeatureSelector` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Steps 1-3 Code so far\n",
    "#### TODO(npragin): Remove these two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Choose a Model\n",
    "\n",
    "### Pick a simple model (show different ones)\n",
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "### Fit model on training data\n",
    "### hyperparameter tuning methods (gridsearch, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 7. Save/Load Models\n",
    "\n",
    "Once you've trained a model, you might want to save it for later use without needing to retrain it. Scikit-learn provides many different ways to save and load models, but we will use `pickle`, a built-in Python library for serializing and deserializing Python objects, because it comes standard with every Python installation.\n",
    "\n",
    "If you are working with large models, consider using `ONNX` or `joblib`. You can find documentation on these methods in the [scikit-learn model persistence documentation](https://scikit-learn.org/stable/model_persistence.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Saving a Model Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "with open(\"saved_model.pkl\", \"wb\") as file:\n",
    "    dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Loading a Model Using Pickle\n",
    "When you use pickle, you can load the model and use it without any knowledge of the model. Pickle encodes everything about the object so you can simply load it and use it without initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "with open(\"saved_model.pkl\", \"rb\") as file:\n",
    "    loaded_model = load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Evaluate the Model\n",
    "### Predict on test data\n",
    "The time has finally come to use our test data! Recall, we use validation and training data to tune our model and hyperparameters; the test data is never-before-seen data we will use to evaluate our model's performance.\n",
    "\n",
    "As we've seen, predicting on test data is easy, but how do we get something meaningful out of it? We can use various evaluation metrics to understand how well our model is performing.\n",
    "\n",
    "### Accuracy scoring\n",
    "The simplest way to measure the performance of our model is to calculate its accuracy. This is simple using the `accuracy_score` function from `sklearn.metrics`.\n",
    "\n",
    "However, accuracy alone can be misleading, especially with imbalanced datasets. For example, if 95% of your data belongs to one class, a model that always predicts that class will have 95% accuracy but is not useful. So, it is always good to compare your accuracy against a baseline model, such as one that makes random predictions or always predicts the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random_classifier = DummyClassifier(strategy=\"uniform\", random_state=42) # Model that will guess randomly\n",
    "random_classifier.fit(X_train, y_train)\n",
    "random_classifier_test_predictions = random_classifier.predict(X_test)\n",
    "\n",
    "most_frequent_classifier = DummyClassifier(strategy=\"most_frequent\") # Model that will always guess the most frequent class\n",
    "most_frequent_classifier.fit(X_train, y_train)\n",
    "most_frequent_classifier_test_predictions = most_frequent_classifier.predict(X_test)\n",
    "\n",
    "loaded_model_test_predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(\"Random Classifier Accuracy:\", accuracy_score(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Accuracy:\", accuracy_score(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Accuracy:\", accuracy_score(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "One flaw with using accuracy scoring is it assumes all errors (false positives and false negatives) are equally bad. In many applications, this is not the case. For example, in medical diagnosis, a false negative (failing to identify a disease) can be much more serious than a false positive (incorrectly diagnosing a disease).\n",
    "\n",
    "To get a better understanding of our model's performance, we can use a **confusion matrix**. A confusion matrix is a table that shows us the number of correct and incorrect predictions made by our model, broken down by each class. It provides a more detailed view of how our model is performing across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Random Classifier Confusion Matrix:\\n\", confusion_matrix(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Confusion Matrix:\\n\", confusion_matrix(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Confusion Matrix:\\n\", confusion_matrix(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Classification reporting\n",
    "\n",
    "A classification report provides a detailed summary of many metrics such as precision (accuracy when predicting a particular class), recall (ability correctly predict a particular class), and F1-score (balance between precision and recall).\n",
    "\n",
    "Precision is useful when you are more concerned about the accuracy of positive predictions, while recall is important when you want to capture as many positive instances as possible. Consider which metric is most useful for your specific application.\n",
    "\n",
    "A medical diagnosis model might prioritize recall to ensure that as many cases of a disease are identified as possible, even if it means some false positives. On the other hand, a spam detection system might prioritize precision to avoid incorrectly marking legitimate emails as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Random Classifier Classification Report:\\n\", classification_report(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Classification Report:\\n\", classification_report(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Classification Report:\\n\", classification_report(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Find Your Own Metrics\n",
    "Check out the metrics supported by scikit-learn [here]() and some more information about them [here]()! Try using one in the code cell below and see how your model compares to the baselines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ___\n",
    "\n",
    "print(\"Random Classifier Classification Report:\\n\", ___)\n",
    "print(\"Most Frequent Classifier Classification Report:\\n\", ___)\n",
    "print(\"Our Model Classification Report:\\n\", ___)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
