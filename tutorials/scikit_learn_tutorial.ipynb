{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scikit-learn Tutorial\n",
    "Author: Lauren Gliane\n",
    "\n",
    "In this tutorial, we'll go over the following topics to build our own classification model:\n",
    "1. Introduction\n",
    "2. Install\n",
    "3. Datasets\n",
    "4. Data Preprocessing\n",
    "5. Choosing a Model\n",
    "6. Training the Model\n",
    "7. Save/Load Models\n",
    "8. Making Predictions and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Get to know SK-learn\n",
    "### What is Scikit-learn?\n",
    "Scikit-learn (AKA sk-learn) is written in Python, an open source project, and is **one of the most used ML libraries today**. Sk-learn is built on top of Numpy, SciPy, and Matplotlib, and contains tons of algorithms ready to use to train, evaluate, and save models straight out of the box!\n",
    "\n",
    "### Why learn Scikit-learn?\n",
    "With sk-learn, we don’t need to implement complex algorithms built on a backbone of linear algebra and statistics. By using sk-learn’s ML algorithms and neural networks, we can build models faster while getting familiar with industry-standard tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "To use sk-learn, we'll need **scipy**, **numpy**, and **sklearn**. To install these, run `pip install scipy numpy scikit-learn` in your terminal.\n",
    "\n",
    "You can confirm sk-learn was installed correctly by importing something from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "When doing machine learning, we need data to train and evaluate our models, because without data, we can't learn patterns, validate performance, or generalize to unseen examples.\n",
    "\n",
    "Scikit-learn provides tools to do that via built-in datasets, dataset loading utilities, and data preprocessing functions (like [train_test_split](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com)).\n",
    "\n",
    "### Features and Labels\n",
    "In machine learning, we use data to train models to make predictions or decisions. This data is typically structured into two main parts:\n",
    "1. **Features (Input)**\n",
    "- Features are the input variables (also called independent variables) that the model uses to learn.\n",
    "- Think of them as the measurable properties or characteristics representing what causes or correlates to your output\n",
    "- Features are real numbers or are non-numeric and have been transformed into a numerical representation\n",
    "\n",
    "    **Example:** In a house price prediction dataset, features might include the number of bedrooms, square footage, and location.\n",
    "\n",
    "2. **Labels (Output)**\n",
    "- The label is the target variable (also called the dependent variable). Labels are what you want the model to predict.\n",
    "- Think of it as the correct answer for each example in the dataset.\n",
    "\n",
    "    **Example:** For house prices, the label is the actual price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 1: Pick a Dataset\n",
    "Scikit-learn comes with several built-in toy datasets that are great for learning and experimenting. These datasets are small, well-structured, and easy to load making them perfect for learning.\n",
    "\n",
    "Look through the following [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and select one that interests you for this tutorial!\n",
    "\n",
    "**Note**: Don't choose the digits dataset, which is an image dataset that requires additional preprocessing this tutorial does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import ______\n",
    "\n",
    "dataset = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Viewing your data\n",
    "Whenever you're working with new data, it's always a good idea to familiarize yourself with the feature names, your labels, and examine a bit of the data. Run the following to understand a bit about the dataset you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data \n",
    "y = dataset.target \n",
    "  \n",
    "feature_names = dataset.feature_names \n",
    "target_names = dataset.target_names \n",
    "  \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Want to use custom data?\n",
    "If we’re using an external dataset, we can use the pandas library to load and manipulate the datasets with ease. If you haven’t yet, check out our [AI Club Pandas Tutorial](https://github.com/npragin/ai-club-project-management/blob/main/tutorials/pandas-tutorial.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "When working with real-world data, it often requires some preprocessing to ensure it's in the right format for training a model. This can include handling missing values, scaling features, and selecting the relevant features.\n",
    "\n",
    "The first step is always to inspect your data to understand its structure and identify any potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### A. Missing Values\n",
    "Missing values can occur for various reasons, such as data entry errors, sensor malfunctions, or respondents skipping questions in surveys. Handling missing values is crucial because they can hinder the performance of machine learning models. Sometimes, datasets will use NaN (Not a Number), null, or zero to represent missing values. It is important to read about your dataset to understand how missing values are represented.\n",
    "\n",
    "#### Identify Missing Data\n",
    "We'll use Pandas to find missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "y = pd.Series(y)\n",
    "\n",
    "print(X.isnull().sum())  # Shows number of nulls per column\n",
    "print()\n",
    "print((X == 0).sum())  # Shows number of zeros per column\n",
    "print()\n",
    "X.info()          # Also shows counts of non-null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Strategies for Handling Missing Values\n",
    "\n",
    "\n",
    "**Option 1: Remove Missing Values**\n",
    "- Drop rows (examples) with missing values, and don't predict unless all features are available\n",
    "  - Useful when a few rows are missing, and you have a large dataset\n",
    "  - `df.dropna(inplace=True)`\n",
    "\n",
    "- Drop columns (features), and don't use that feature for training or prediction\n",
    "  - Useful when a feature has many missing values and is not critical to the task\n",
    "  - `df.drop(columns=['column_name'], inplace=True)`\n",
    "\n",
    "**Option 2: Imputation (Fill Missing Values)**\n",
    "\n",
    "[SK-Learn imputers](https://scikit-learn.org/stable/api/sklearn.impute.html)\n",
    "\n",
    "- Fill missing values with a specific value, like the mean, median, or mode of the column\n",
    "  - `SimpleImputer`\n",
    "- Fit a function to the non-missing values, then use that function to fill in the missing values\n",
    "  - `IterativeImputer`\n",
    "- Use a KNN model to predict and fill in the missing values\n",
    "  - `KNNImputer`\n",
    "\n",
    "#### Step 2: Fill Missing Values (if needed)\n",
    "If you found your dataset has missing values, choose one of the strategies above to handle them. If not, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import ____\n",
    "imputer = ____\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "| Scaler           | Use When                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------- |\n",
    "| `StandardScaler` | You want features with mean = 0 and std = 1 (default for most ML models). |\n",
    "| `MinMaxScaler`   | You need values in a fixed range, like \\[0, 1].                           |\n",
    "| `RobustScaler`   | Your data contains **outliers**.                                          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### C. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset will be split into the training set and testing set.\n",
    "\n",
    "- *Training set:* teaches our model to recognize patterns in the data\n",
    "- *Testing set:* checks our model’s performance on new, never seen before data\n",
    "\n",
    "We will use the [train_test_split()](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn.model_selection module to do this.\n",
    "\n",
    "##### Deciding on a split\n",
    "80% training and 20% testing data is the most common split for larger datasets. \n",
    "Since most of sk-learn's toy datasets are small, we’ll use 60% for training and 40% for testing to ensure we can be confident about our evaluation results. \n",
    "\n",
    "To do this, the parameter responsible for train size or test size (either or both) by taking a look at the docs to see how to pass them in. By adding setting `random_state=1`, we can ensure the split is consistent with each run for reproducibility.\n",
    "\n",
    "**Result:**\n",
    "We will have four subsets of the data after splitting.\n",
    "\n",
    "- **x_train and y_train:** feature and target values for training\n",
    "\n",
    "- **x_test and y_test:** feature and target values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, random_state=1, ___) # Fill in parameter(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Other Common Steps in Data Preparation with Scikit-learn tools\n",
    "For this tutorial, we will only go over loading data, scaling, and splitting. Below shows the common steps for preprocessing and sk-learn tools used to complete them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "| Step                                   | What It Does                                                    | Example Tools in Scikit-learn         |\n",
    "| -------------------------------------- | --------------------------------------------------------------- | ------------------------------------- |\n",
    "| **Encode Categorical Variables**    | Convert text labels into numbers.                               | `OneHotEncoder`, `LabelEncoder`       |\n",
    "| **Feature Selection / Engineering** | Choose the features that maximize performance.             | `SelectKBest`, `SequentialFeatureSelector` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Steps 1-3 Code so far\n",
    "#### TODO(npragin): Remove these two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Choose a Model\n",
    "\n",
    "### Pick a simple model (show different ones)\n",
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "### Fit model on training data\n",
    "### hyperparameter tuning methods (gridsearch, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 7. Save/Load Models\n",
    "\n",
    "Once you've trained a model, you might want to save it for later use without needing to retrain it. Scikit-learn provides many different ways to save and load models, but we will go over `pickle`, a built-in Python library for serializing and deserializing Python objects.\n",
    "\n",
    "If you are working with large models, consider using `ONNX` or `joblib`. You can find documentation on these methods in the [scikit-learn model persistence documentation](https://scikit-learn.org/stable/model_persistence.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Saving a Model Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "with open(\"saved_model.pkl\", \"wb\") as file:\n",
    "    dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb604f8",
   "metadata": {},
   "source": [
    "### Loading a Model Using Pickle\n",
    "When you use pickle, you can load the model and use it without any knowledge of the model. Pickle encodes everything about the object so you can simply load it and use it without initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "with open(\"saved_model.pkl\", \"rb\") as file:\n",
    "    loaded_model = load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Evaluate the Model\n",
    "### Predict on test data\n",
    "### Accuracy scoring\n",
    "### Confusion matrix\n",
    "### Classification reporting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
