{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Scikit-learn Tutorial\n",
    "Author: Lauren Gliane\n",
    "\n",
    "In this tutorial, we'll go over the following topics to build our own classification model:\n",
    "1. Introduction\n",
    "2. Install\n",
    "3. Datasets\n",
    "4. Data Preprocessing\n",
    "5. Choosing a Model\n",
    "6. Training the Model\n",
    "7. Save/Load Models\n",
    "8. Making Predictions and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Get to know SK-learn\n",
    "### What is Scikit-learn?\n",
    "Scikit-learn (AKA sk-learn) is written in Python, an open source project, and is **one of the most used ML libraries today**. Sk-learn is built on top of Numpy, SciPy, and Matplotlib, and contains tons of algorithms ready to use to train, evaluate, and save models straight out of the box!\n",
    "\n",
    "### Why learn Scikit-learn?\n",
    "With sk-learn, we don’t need to implement complex algorithms built on a backbone of linear algebra and statistics. By using sk-learn’s ML algorithms and neural networks, we can build models faster while getting familiar with industry-standard tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "To use sk-learn, we'll need **scipy**, **numpy**, and **sklearn**. To install these, run `pip install scipy numpy scikit-learn` in your terminal.\n",
    "\n",
    "You can confirm sk-learn was installed correctly by importing something from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "When doing machine learning, we need data to train and evaluate our models, because without data, we can't learn patterns, validate performance, or generalize to unseen examples.\n",
    "\n",
    "Scikit-learn provides tools to do that via built-in datasets, dataset loading utilities, and data preprocessing functions (like [train_test_split](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com)).\n",
    "\n",
    "### Features and Labels\n",
    "In machine learning, we use data to train models to make predictions or decisions. This data is typically structured into two main parts:\n",
    "1. **Features (Input)**\n",
    "- Features are the input variables (also called independent variables) that the model uses to learn.\n",
    "- Think of them as the measurable properties or characteristics representing what causes or correlates to your output\n",
    "- Features are real numbers or are non-numeric and have been transformed into a numerical representation\n",
    "\n",
    "    **Example:** In a house price prediction dataset, features might include the number of bedrooms, square footage, and location.\n",
    "\n",
    "2. **Labels (Output)**\n",
    "- The label is the target variable (also called the dependent variable). Labels are what you want the model to predict.\n",
    "- Think of it as the correct answer for each example in the dataset.\n",
    "\n",
    "    **Example:** For house prices, the label is the actual price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Step 1: Pick a Dataset\n",
    "Scikit-learn comes with several built-in toy datasets that are great for learning and experimenting. These datasets are small, well-structured, and easy to load making them perfect for learning.\n",
    "\n",
    "Look through the following [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and select one that interests you for this tutorial!\n",
    "\n",
    "**Note**: Don't choose the digits dataset, which is an image dataset that requires additional preprocessing this tutorial does not support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import ______\n",
    "\n",
    "dataset = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "##### Viewing your data\n",
    "Whenever you're working with new data, it's always a good idea to familiarize yourself with the feature names, your labels, and examine a bit of the data. Run the following to understand a bit about the dataset you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data \n",
    "y = dataset.target \n",
    "  \n",
    "feature_names = dataset.feature_names \n",
    "target_names = dataset.target_names \n",
    "  \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Want to use custom data?\n",
    "If we’re using an external dataset, we can use the pandas library to load and manipulate the datasets with ease. If you haven’t yet, check out our [AI Club Pandas Tutorial](https://github.com/npragin/ai-club-project-management/blob/main/tutorials/pandas-tutorial.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "When working with real-world data, it often requires some preprocessing to ensure it's in the right format for training a model. This can include handling missing values, scaling features, and selecting the relevant features.\n",
    "\n",
    "The first step is always to inspect your data to understand its structure and identify any potential issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### A. Missing Values\n",
    "Missing values can occur for various reasons, such as data entry errors, sensor malfunctions, or respondents skipping questions in surveys. Handling missing values is crucial because they can hinder the performance of machine learning models. Sometimes, datasets will use NaN (Not a Number), null, or zero to represent missing values. It is important to read about your dataset to understand how missing values are represented.\n",
    "\n",
    "#### Identify Missing Data\n",
    "We'll use Pandas to find missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "y = pd.Series(y)\n",
    "\n",
    "print(X.isnull().sum())  # Shows number of nulls per column\n",
    "print()\n",
    "print((X == 0).sum())  # Shows number of zeros per column\n",
    "print()\n",
    "X.info()          # Also shows counts of non-null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Strategies for Handling Missing Values\n",
    "\n",
    "\n",
    "**Option 1: Remove Missing Values**\n",
    "- Drop rows (examples) with missing values, and don't predict unless all features are available\n",
    "  - Useful when a few rows are missing, and you have a large dataset\n",
    "  - `df.dropna(inplace=True)`\n",
    "\n",
    "- Drop columns (features), and don't use that feature for training or prediction\n",
    "  - Useful when a feature has many missing values and is not critical to the task\n",
    "  - `df.drop(columns=['column_name'], inplace=True)`\n",
    "\n",
    "**Option 2: Imputation (Fill Missing Values)**\n",
    "\n",
    "[SK-Learn imputers](https://scikit-learn.org/stable/api/sklearn.impute.html)\n",
    "\n",
    "- Fill missing values with a specific value, like the mean, median, or mode of the column\n",
    "  - `SimpleImputer`\n",
    "- Fit a function to the non-missing values, then use that function to fill in the missing values\n",
    "  - `IterativeImputer`\n",
    "- Use a KNN model to predict and fill in the missing values\n",
    "  - `KNNImputer`\n",
    "\n",
    "#### Step 2: Fill Missing Values (if needed)\n",
    "If you found your dataset has missing values, choose one of the strategies above to handle them. If not, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import ____\n",
    "imputer = ____\n",
    "X = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "\n",
    "**Standardization** -- `StandardScaler`\n",
    "This scales features so they have zero mean and unit variance.\n",
    "\n",
    "| Use `StandardScaler` when:                                                      |\n",
    "| ------------------------------------------------------------------------------- |\n",
    "| ✅ Your features are **normally distributed** or approximately so                |\n",
    "| ✅ You are using models that assume features are **centered around zero**, like: |\n",
    "|     • Logistic Regression                                                       |\n",
    "|     • Support Vector Machines (SVM)                                             |\n",
    "|     • K-Nearest Neighbors (KNN)                                                 |\n",
    "|     • Principal Component Analysis (PCA)                                        |\n",
    "| ✅ You want to maintain **variance structure** of the data                       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5c4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on data and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530fe5be",
   "metadata": {},
   "source": [
    "**Normalization** -- `MinMaxScaler`\n",
    "This scales features to a fixed range, usually [0,1]:\n",
    "\n",
    "| Use `MinMaxScaler` when:                                                                                  |\n",
    "| --------------------------------------------------------------------------------------------------------- |\n",
    "| ✅ You need your features scaled to a **specific range**, like `[0, 1]` or `[-1, 1]`                       |\n",
    "| ✅ Your data is **uniformly distributed** without significant outliers                                     |\n",
    "| ✅ You are using algorithms that are **sensitive to the scale and bounded input**, such as neural networks |\n",
    "| ✅ You are working with image data or input that needs normalization to a known range                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "X = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on data and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b427a78",
   "metadata": {},
   "source": [
    "**Robust Scaling** -- `Robust Scaler`\n",
    "\n",
    "This scales features using the median and interquartile range (IQR), making it robust to outliers.\n",
    "| Use `RobustScaler` when:                                   |\n",
    "| ---------------------------------------------------------- |\n",
    "| ✅ Your data contains **outliers**                          |\n",
    "| ✅ You want to reduce the effect of extreme values          |\n",
    "| ✅ You're dealing with skewed or non-Gaussian distributions |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f22af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Sample data with outliers\n",
    "X = [[1, 2], [2, 3], [3, 4], [100, 200]]\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Fit on data and transform\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### C. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset will be split into the training set and testing set.\n",
    "\n",
    "- *Training set:* teaches our model to recognize patterns in the data\n",
    "- *Testing set:* checks our model’s performance on new, never seen before data\n",
    "\n",
    "We will use the [train_test_split()](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from sklearn.model_selection module to do this.\n",
    "\n",
    "##### Deciding on a split\n",
    "80% training and 20% testing data is the most common split for larger datasets. \n",
    "Since most of sk-learn's toy datasets are small, we’ll use 60% for training and 40% for testing to ensure we can be confident about our evaluation results. \n",
    "\n",
    "To do this, the parameter responsible for train size or test size (either or both) by taking a look at the docs to see how to pass them in. By adding setting `random_state=1`, we can ensure the split is consistent with each run for reproducibility.\n",
    "\n",
    "**Result:**\n",
    "We will have four subsets of the data after splitting.\n",
    "\n",
    "- **x_train and y_train:** feature and target values for training\n",
    "\n",
    "- **x_test and y_test:** feature and target values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, random_state=1, ___) # Fill in parameter(s) here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Other Common Steps in Data Preparation with Scikit-learn tools\n",
    "For this tutorial, we will only go over loading data, scaling, and splitting. Below shows the common steps for preprocessing and sk-learn tools used to complete them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "\n",
    "| Step                                   | What It Does                                                    | Example Tools in Scikit-learn         |\n",
    "| -------------------------------------- | --------------------------------------------------------------- | ------------------------------------- |\n",
    "| **Encode Categorical Variables**    | Convert text labels into numbers.                               | `OneHotEncoder`, `LabelEncoder`       |\n",
    "| **Feature Selection / Engineering** | Choose the features that maximize performance.             | `SelectKBest`, `SequentialFeatureSelector` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Steps 1-3 Code so far\n",
    "#### TODO(npragin): Remove these two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 5. Choose a Model\n",
    "\n",
    "### Where to Find Models\n",
    "Choosing the right model depends on your data type, problem type (classification, regression, clustering), and performance needs.\n",
    "1. **scikit-learn Library**\n",
    "Scikit-learn includes a wide range of built-in models for classification, regression, and clustering. You can browse all available models here: https://scikit-learn.org/stable/supervised_learning.html\n",
    "\n",
    "| Task           | Common Models in `sklearn`                                                    |\n",
    "| -------------- | ----------------------------------------------------------------------------- |\n",
    "| Classification | `LogisticRegression`, `RandomForestClassifier`, `SVC`, `KNeighborsClassifier` |\n",
    "| Regression     | `LinearRegression`, `RandomForestRegressor`, `SVR`, `Ridge`, `Lasso`          |\n",
    "| Clustering     | `KMeans`, `DBSCAN`, `AgglomerativeClustering`                                 |\n",
    "\n",
    "2. **Model Selection Cheatsheets**\n",
    "Scikit-learn provides this helpful flowchart to pick models:\n",
    "https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "It walks you through choosing based on data size, feature count, and task type.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Cross-validation helps you evaluate how a model generalizes to unseen data.\n",
    "\n",
    "What is Cross-Validation?\n",
    "\n",
    "Instead of a single train/test split, cross-validation:\n",
    "\n",
    "1. Splits data into k folds (e.g., 5 or 10)\n",
    "\n",
    "2. Trains the model on k-1 folds\n",
    "\n",
    "3. Tests it on the remaining fold\n",
    "\n",
    "4. Repeats for all folds and averages the result\n",
    "\n",
    "**Types of Cross-Validation**\n",
    "| Type              | When to Use                                |\n",
    "| ----------------- | ------------------------------------------ |\n",
    "| `KFold`           | General-purpose                            |\n",
    "| `StratifiedKFold` | For classification with imbalanced classes |\n",
    "| `TimeSeriesSplit` | For time-series data (preserves order)     |\n",
    "\n",
    "### Tips for Choosing a Model\n",
    "**Start Simple:**\n",
    "- Begin with Logistic Regression or Decision Tree\n",
    "- Easy to interpret, quick to train\n",
    "\n",
    "**Use Evaluation Metrics:**\n",
    "\n",
    "| Task           | Metric                      |\n",
    "| -------------- | --------------------------- |\n",
    "| Classification | Accuracy, F1-score, ROC AUC |\n",
    "| Regression     | MAE, RMSE, R²               |\n",
    "| Clustering     | Silhouette score, inertia   |\n",
    "\n",
    "\n",
    "Use cross_val_score, GridSearchCV, or RandomizedSearchCV to compare models.\n",
    "\n",
    "**Consider Trade-offs:**\n",
    "| Factor           | Question to Ask                         |\n",
    "| ---------------- | --------------------------------------- |\n",
    "| Accuracy         | Is the model’s performance good enough? |\n",
    "| Speed            | Is training or inference time critical? |\n",
    "| Interpretability | Do I need to explain the model?         |\n",
    "| Data Size        | Can the model scale to my dataset?      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "The objective of Model Training is to minimize loss or error on training data. The overview of the model lifecycle is train, validate, test.\n",
    "\n",
    "There are different types of Machine Learning Models:\n",
    "- Supervised: Regression, Classification\n",
    "- Unsupervised: Clustering, Dimensionality Reduction\n",
    "- Algorithms: Linear Regression, Decision \n",
    "\n",
    "### Fitting Models on Training Data\n",
    "Fitting involves training the model on the training data using `.fit(X_train, y_train)`. It learns the mapping from features to target.\n",
    "\n",
    "Here is an example of fitting for Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e000c1b4",
   "metadata": {},
   "source": [
    "### Training vs Validation Performance\n",
    "After fitting, assess performance on both the training and validation/test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de89e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241c4ca",
   "metadata": {},
   "source": [
    "#### Watch out for Underfitting and Overfitting:\n",
    "- **Underfitting:** Low accuracy on train set, Low test set accuracy\n",
    "- **Overfitting:** High accuracy on train set, Low test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84bd18e",
   "metadata": {},
   "source": [
    "# -- add a common mistakes section (??) --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0ae28",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "#### What are Hyperparameters?\n",
    "Hyperparameters are variables set before training a model. Unlike model parameters (which are learned from data), hyperparameters are manually set and controls the model's learning process.\n",
    "\n",
    "#### Why Are Hyperparameters Important?\n",
    "Hyperparameters are important to ML since they:\n",
    "\n",
    "- Affect model complexity (ex: tree depth)\n",
    "- Determine how fast or how well a model learns (ex: learning rate)\n",
    "- Influence overfitting vs underfitting\n",
    "- Impact training time and computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbf7d7",
   "metadata": {},
   "source": [
    "### Hyperparameter Types\n",
    "1. **Model Complexity Hyperparameters**\n",
    "- Examples: max_depth, C, n_neighbors\n",
    "- Controls how complex a model can get\n",
    "- More complexity → lower bias, higher variance\n",
    "\n",
    "2. **Training Control Hyperparameters**\n",
    "- Examples: learning_rate, batch_size, epochs\n",
    "- Controls how the model is trained\n",
    "- Lower learning rate → slower, possibly more accurate convergence\n",
    "\n",
    "3. **Regularization Hyperparameters**\n",
    "- Examples: alpha, C, l1_ratio\n",
    "- Penalizes model complexity to prevent overfitting\n",
    "\n",
    "### Step 1: Create a Parameter Grid\n",
    "Make a dictionary of hyperparameters to test. Best to start small, and expand if needed.\n",
    "\n",
    "**Tips:**\n",
    "- Use 2-4 values per parameter\n",
    "- Avoid too wide of a range unless using random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae056f",
   "metadata": {},
   "source": [
    "### Step 2: Pick Your Hyperparameter Tuning Strategy\n",
    "| Method                    | Description                                  | Best For                           |\n",
    "| ------------------------- | -------------------------------------------- | ---------------------------------- |\n",
    "| **Grid Search**           | Tries all combinations from a param grid     | Small search spaces                |\n",
    "| **Random Search**         | Samples random combinations from the grid    | Large search spaces                |\n",
    "| **Bayesian Optimization** | Uses past results to choose next best values | Smart, efficient tuning            |\n",
    "| **Manual tuning**         | Trial and error                              | Simple models or quick prototyping |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e35390",
   "metadata": {},
   "source": [
    "### Step 3: Use Cross-Validation\n",
    "**Cross-validation** is a technique used to evaluate the performance of a machine learning model by splitting the training data into multiple smaller parts — to train on some and validate on others — in a rotating fashion.\n",
    "\n",
    "Goal: Provide a more reliable estimate of how the model will perform on unseen data.\n",
    "\n",
    "Help prevent overfitting during model selection or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7ac5f",
   "metadata": {},
   "source": [
    "#### Cross-Validation Practice in scikit-learn\n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69324255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"Accuracy scores on each fold:\", scores)\n",
    "print(\"Mean cross-validation accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ec5f3",
   "metadata": {},
   "source": [
    "#### Cross-Validation During Hyperparameter Tuning\n",
    "When you use GridSearchCV or RandomizedSearchCV, cross-validation is built-in.\n",
    "\n",
    "- cv=5: The grid will train+validate on 5 different splits for each parameter combination\n",
    "- best_score_ is the average CV score for the best-performing combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e79ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5, 7]}\n",
    "grid = GridSearchCV(model, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score (CV average):\", grid.best_score_)\n",
    "print(\"Best params:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b948f4f",
   "metadata": {},
   "source": [
    "### Common Tuning Mistakes\n",
    "| Mistake                      | Description                                     | How to Avoid                           |\n",
    "| ---------------------------- | ----------------------------------------------- | -------------------------------------- |\n",
    "| Using default values blindly | Defaults may not suit your dataset              | Always perform hyperparameter tuning   |\n",
    "| Tuning on test data          | Leads to data leakage                           | Use cross-validation or validation set |\n",
    "| Over-tuning                  | Too many parameters → overfitting on validation | Keep tuning space sensible             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 7. Save/Load Models\n",
    "\n",
    "Once you've trained a model, you might want to save it for later use without needing to retrain it. Scikit-learn provides many different ways to save and load models, but we will use `pickle`, a built-in Python library for serializing and deserializing Python objects, because it comes standard with every Python installation.\n",
    "\n",
    "If you are working with large models, consider using `ONNX` or `joblib`. You can find documentation on these methods in the [scikit-learn model persistence documentation](https://scikit-learn.org/stable/model_persistence.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Saving a Model Using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "with open(\"saved_model.pkl\", \"wb\") as file:\n",
    "    dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Loading a Model Using Pickle\n",
    "When you use pickle, you can load the model and use it without any knowledge of the model. Pickle encodes everything about the object so you can simply load it and use it without initializing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "with open(\"saved_model.pkl\", \"rb\") as file:\n",
    "    loaded_model = load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 8. Make Predictions and Evaluate the Model\n",
    "### Predict on test data\n",
    "The time has finally come to use our test data! Recall, we use validation and training data to tune our model and hyperparameters; the test data is never-before-seen data we will use to evaluate our model's performance.\n",
    "\n",
    "As we've seen, predicting on test data is easy, but how do we get something meaningful out of it? We can use various evaluation metrics to understand how well our model is performing.\n",
    "\n",
    "### Accuracy scoring\n",
    "The simplest way to measure the performance of our model is to calculate its accuracy. This is simple using the `accuracy_score` function from `sklearn.metrics`.\n",
    "\n",
    "However, accuracy alone can be misleading, especially with imbalanced datasets. For example, if 95% of your data belongs to one class, a model that always predicts that class will have 95% accuracy but is not useful. So, it is always good to compare your accuracy against a baseline model, such as one that makes random predictions or always predicts the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random_classifier = DummyClassifier(strategy=\"uniform\", random_state=42) # Model that will guess randomly\n",
    "random_classifier.fit(X_train, y_train)\n",
    "random_classifier_test_predictions = random_classifier.predict(X_test)\n",
    "\n",
    "most_frequent_classifier = DummyClassifier(strategy=\"most_frequent\") # Model that will always guess the most frequent class\n",
    "most_frequent_classifier.fit(X_train, y_train)\n",
    "most_frequent_classifier_test_predictions = most_frequent_classifier.predict(X_test)\n",
    "\n",
    "loaded_model_test_predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(\"Random Classifier Accuracy:\", accuracy_score(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Accuracy:\", accuracy_score(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Accuracy:\", accuracy_score(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "One flaw with using accuracy scoring is it assumes all errors (false positives and false negatives) are equally bad. In many applications, this is not the case. For example, in medical diagnosis, a false negative (failing to identify a disease) can be much more serious than a false positive (incorrectly diagnosing a disease).\n",
    "\n",
    "To get a better understanding of our model's performance, we can use a **confusion matrix**. A confusion matrix is a table that shows us the number of correct and incorrect predictions made by our model, broken down by each class. It provides a more detailed view of how our model is performing across different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Random Classifier Confusion Matrix:\\n\", confusion_matrix(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Confusion Matrix:\\n\", confusion_matrix(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Confusion Matrix:\\n\", confusion_matrix(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Classification reporting\n",
    "\n",
    "A classification report provides a detailed summary of many metrics such as precision (accuracy when predicting a particular class), recall (ability correctly predict a particular class), and F1-score (balance between precision and recall).\n",
    "\n",
    "Precision is useful when you are more concerned about the accuracy of positive predictions, while recall is important when you want to capture as many positive instances as possible. Consider which metric is most useful for your specific application.\n",
    "\n",
    "A medical diagnosis model might prioritize recall to ensure that as many cases of a disease are identified as possible, even if it means some false positives. On the other hand, a spam detection system might prioritize precision to avoid incorrectly marking legitimate emails as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Random Classifier Classification Report:\\n\", classification_report(y_test, random_classifier_test_predictions))\n",
    "print(\"Most Frequent Classifier Classification Report:\\n\", classification_report(y_test, most_frequent_classifier_test_predictions))\n",
    "print(\"Our Model Classification Report:\\n\", classification_report(y_test, loaded_model_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Find Your Own Metrics\n",
    "Check out the metrics supported by scikit-learn [here]() and some more information about them [here]()! Try using one in the code cell below and see how your model compares to the baselines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ___\n",
    "\n",
    "print(\"Random Classifier Classification Report:\\n\", ___)\n",
    "print(\"Most Frequent Classifier Classification Report:\\n\", ___)\n",
    "print(\"Our Model Classification Report:\\n\", ___)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
