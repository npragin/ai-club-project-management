{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e56f5d",
   "metadata": {},
   "source": [
    "# Scikit-learn Tutorial\n",
    "Author: Lauren Gliane\n",
    "\n",
    "In this tutorial, we'll go over the following topics to build our own classification model:\n",
    "1. Introduction\n",
    "2. Install\n",
    "3. Datasets\n",
    "4. Choosing a Model\n",
    "5. Training the Model\n",
    "6. Making Predictions\n",
    "7. Save/Load Models\n",
    "8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912cbe6c",
   "metadata": {},
   "source": [
    "## 1. Get to know SK-learn\n",
    "### What is Scikit-learn?\n",
    "Scikit-learn (AKA sk-learn) is written in Python, an open source project, and is **one of the most used ML libraries today**. Sklearn contains modules built on top of Numpy, SciPy, and Matplotlib libraries containing tons of algorithms--ready to use to train, evaluate, and save models straight out of the box!\n",
    "\n",
    "### Why learn Scikit-learn?\n",
    "With sk-learn, we don’t need an in-depth understanding of complex concepts like linear algebra or calculus. By using sk-learn’s pretrained neural networks and ML algorithms, we can easily preprocess datasets for supervised learning (regression or classification) and unsupervised learning (clustering and dimensionality reduction) applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7a001",
   "metadata": {},
   "source": [
    "## 2. Install\n",
    "To use sk-learn, we'll need **scipy**, **numpy**, and **sklearn**. To install these, run `pip install scipy numpy scikit-learn` in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db5c4c",
   "metadata": {},
   "source": [
    "## 3. Datasets\n",
    "When doing machine learning, we need data to train and evaluate our models, because without data, we can't learn patterns, validate performance, or generalize to unseen examples.\n",
    "\n",
    "Scikit-learn provides tools to do that via built-in datasets, dataset loading utilities, and data preprocessing functions (like [train_test_split](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com)).\n",
    "\n",
    "### Features and Labels\n",
    "In machine learning, we use data to train models to make predictions or decisions. This data is typically structured into two main parts:\n",
    "1. **Features (Input)**\n",
    "- Features are the input variables (also called independent variables) that the model uses to learn.\n",
    "- Think of them as the measurable properties or characteristics of the data\n",
    "\n",
    "    **Example:** In a house price prediction dataset, features might include the number of bedrooms, square footage, and location.\n",
    "\n",
    "2. **Labels (Output)**\n",
    "- The label is the target variable (also called the dependent variable) — it’s what you want the model to predict.\n",
    "- Think of it as: The correct answer for each example in the dataset.\n",
    "\n",
    "    **Example:** For house prices, the label is the actual price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f5bcd",
   "metadata": {},
   "source": [
    "### Step 1: Pick a Dataset\n",
    "Scikit-learn comes with several built-in toy datasets that are great for learning and experimenting. These datasets are small, well-structured, and easy to load — perfect for understanding the basics of machine learning workflows.\n",
    "\n",
    "Look through the following [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html) and select one that interests you for this tutorial!\n",
    "\n",
    "(We'll be using Iris as an example so we suggest using something else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c1191",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dad50",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "We’ll practice loading datasets with the Iris dataset using `load_iris()`. \n",
    "\n",
    "**About Iris:** Iris dataset will be used for a classification task. The dataset contains measurements of Sepal Length, Sepal Width, Petal Length, and Petal Width, which we will use to identify the iris species.\n",
    "\n",
    "##### Practice: View your data\n",
    "Use this code to view the feature names, target names, feature data, and target data of your selected dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce01964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "\n",
    "iris = load_iris() \n",
    "## load data into x & target into y\n",
    "X = iris.data \n",
    "y = iris.target \n",
    "  \n",
    "feature_names = iris.feature_names \n",
    "target_names = iris.target_names \n",
    "  \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa63ba7",
   "metadata": {},
   "source": [
    "#### Want to use custom data?\n",
    "If we’re using an external dataset, we can use the pandas library to load and manipulate the datasets with ease. If you haven’t yet, check out our [AI Club Pandas Tutorial](https://github.com/npragin/ai-club-project-management/blob/main/tutorials/pandas-tutorial.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad261ad4",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing\n",
    "Raw data is rarely in a form that machine learning models can use effectively. Without data preparation, we risk: Poor performance, Errors or crashes, and Bias or misleading results. \n",
    "\n",
    "In this tutorial, we will go over the most essential steps: **Missing Values**, **Feature Scaling**, and **Splitting Data**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d624d3",
   "metadata": {},
   "source": [
    "#### A. Missing Values (imputation methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715721e5",
   "metadata": {},
   "source": [
    "#### B. Feature Scaling\n",
    "\n",
    "Step 1: Choose your scaling method\n",
    "| Scaler           | Use When                                                                  |\n",
    "| ---------------- | ------------------------------------------------------------------------- |\n",
    "| `StandardScaler` | You want features with mean = 0 and std = 1 (default for most ML models). |\n",
    "| `MinMaxScaler`   | You need values in a fixed range, like \\[0, 1].                           |\n",
    "| `RobustScaler`   | Your data contains **outliers**.                                          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f62ee82",
   "metadata": {},
   "source": [
    "#### C. Split the Data\n",
    "To efficiently train and evaluate model performance, the dataset will be split into the training set and testing set.\n",
    "*Training set:* teaches our model to recognize patterns in the data\n",
    "*Testing set:* checks our model’s performance on new, never seen before data\n",
    "\n",
    "We will use the [train_test_split()](https://sklearn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?utm_source=chatgpt.com) function from sklearn.model_selection module to do this.\n",
    "\n",
    "##### 80-20 Split\n",
    "80% training and 20% testing data is the most common split for larger datasets. \n",
    "Iris is a small dataset containing only 150 samples, so we’ll use 60% for training and 40% for testing. \n",
    "\n",
    "To do this, set parameter `test_size=0.4`. By adding setting `random_state=1`, we can ensure the split is consistent with each run for reproducibility.\n",
    "\n",
    "**Result:** we will get four subsets after splitting\n",
    "\n",
    "**x_train and y_train:** feature and target values for training\n",
    "\n",
    "**x_test and y_test:** feature and target values for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641397bd",
   "metadata": {},
   "source": [
    "#### Steps 1-3 Code so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84171018",
   "metadata": {},
   "source": [
    "## 4. Choose a Model\n",
    "\n",
    "### Pick a simple model (show different ones)\n",
    "### Cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f823d2",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "### Fit model on training data\n",
    "### hyperparameter tuning methods (gridsearch, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce3a34",
   "metadata": {},
   "source": [
    "## 6. Make predictions\n",
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16c416",
   "metadata": {},
   "source": [
    "## 7. Save/Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b44e9e",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model\n",
    "### Accuracy scoring\n",
    "### Confusion matrix\n",
    "### Classification reporting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
