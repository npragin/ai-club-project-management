{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ffd6f93",
   "metadata": {},
   "source": [
    "# PyTorch QuickGuide\n",
    "\n",
    "Author: Kellen Sullivan\n",
    "\n",
    "This is a quick guide for how to load in pre-built models provided by PyTorch to train and deploy on your own on dataset. If you are interested in learning all the PyTorch basics, and how to build your very own model, checkout the official PyTorch begineers guide [here](https://docs.pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493015d",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "To get started using PyTorch, you first have to install it! To do so, open a new terminal and run the following command:\n",
    "\n",
    "- If you are on Windows/Mac: `pip3 install torch torchvision`\n",
    "- If you are on Linux: `pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu`\n",
    "\n",
    "Note that these commands will use your CPU as the compute platform. If you have an NVIDIA GPU, you can install PyTorch with CUDA as the compute platform to greatly speed up training. Check out the official installation guide to learn more: https://pytorch.org/get-started/locally/\n",
    "\n",
    "You can confirm PyTorch was successfully installed by importing the package and printing out its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cbba26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ca10e",
   "metadata": {},
   "source": [
    "## Loading a Prebuilt Model\n",
    "\n",
    "PyTorch has libraries such as [torchvision](https://docs.pytorch.org/vision/0.8/models.html), [torchaudio](https://docs.pytorch.org/audio/stable/models.html), and [torchtext](https://docs.pytorch.org/text/stable/models.html) that provide prebuilt models to quickly process image data, audio data, or text data. In this quickguide, we will use a prebuilt model from torchvision to classify images into 1 of 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548eb457",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "Before we load in a prebuilt model, we need to get data. Thankfully torchvision provides datasets that can be easily accessed to train neural networks in PyTorch, including the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset that we will be training a model on!\n",
    "\n",
    "CIFAR-10 contains small 32×32 pixel color images across 10 categories.\n",
    "However, the prebuilt ResNet-18 model we will later use to classify CIFAR-10 images was originally trained on [ImageNet](https://www.image-net.org/download.php?utm_source=chatgpt.com), a much larger dataset with 224×224 pixel images and 1,000 output classes.\n",
    "\n",
    "Because of these differences, we’ll need to do two things:\n",
    "1. Resize the CIFAR-10 images to match the input size expected by ResNet-18 (224x224).\n",
    "2. Modify the model’s final layer so it predicts 10 classes instead of 1,000. We will do this later in the PreBuilt Models section.\n",
    "\n",
    "To start, we’ll define a transform to resize the CIFAR-10 images and prepare them for use with the ResNet-18 prebuilt model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db76102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # resize images: 32x32 -> 224x224\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\", \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc3a81",
   "metadata": {},
   "source": [
    "We load training data and testing data separately, that way we can train the model using the training data and then test it on never before seen data, to ensure the model is indeed learning how to classify images and not just memorizing the training dataset. \n",
    "\n",
    "`training_data` is a set of 50,000 3x32x32 images with their corresponding labels (3 representes the 3 color channels red, green, and blue)\n",
    "\n",
    "`test_data` is a set of 10,000 3x32x32 images with their corresponding labels \n",
    "\n",
    "The parameters do the following:\n",
    "- `root` is the path where the train/test data is stored\n",
    "- `train` specifies training or test dataset\n",
    "- `download` if true, dowloads the data from the internet if it's not available at `root`.\n",
    "- `transform` specify the feature and label transformations\n",
    "\n",
    "To check that we successfully loaded the data, let's print a random example image from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca8673b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m idx = random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mtraining_data\u001b[49m) - \u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m img, label = training_data[idx]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# show image\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(training_data) - 1)\n",
    "img, label = training_data[idx]\n",
    "\n",
    "# show image\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(training_data.classes[label])\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dea19a",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "Before we can start training, we need an efficient way to feed images from our dataset into our model. Thankfully, PyTorch’s `DataLoader` class makes this simple. It wraps an existing dataset and handles batching, shuffling, and loading data in parallel, which can greatly improve training efficiency. The key difference between a dataset and `DataLoader` is that a dataset provides access to individual samples, while a `DataLoader` controls how those samples are efficiently fed to the model during training.\n",
    "\n",
    "The following code creates two DataLoaders, one for training and one for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    training_data,\n",
    "    batch_size=64, # number of samples loaded per batch\n",
    "    shuffle=True,  # randomize the order of samples\n",
    "    num_workers=2  # number of subprocesses used to load data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f741c",
   "metadata": {},
   "source": [
    "These parameters determine how your data is batched, shuffled, and loaded\n",
    "- `batch_size` is how many samples are loaded per batch.\n",
    "- `shuffle=True` randomizes the sample order each time the data is loaded, preventing the model from memorizing the order.\n",
    "- `num_workers` is the number of subprocesses used to load data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82df0b",
   "metadata": {},
   "source": [
    "### Prebuilt Models\n",
    "\n",
    "Now that we have loaded our data and can iterate through it, we can load in our pre-built model! You can read about many different prebuilt models provided by torchvision [here](https://docs.pytorch.org/vision/0.9/models.html). For this quickguide we will use the [ResNet-18](https://docs.pytorch.org/vision/2.0/models/generated/torchvision.models.resnet18.html) model due to its moderate size and robust feature extraction capabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ec101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "resnet_model = models.resnet18(weights=None) # only load model structure, not model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332a97e",
   "metadata": {},
   "source": [
    "You can view the structure of any PyTorch model by simply printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ef04871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6b767",
   "metadata": {},
   "source": [
    "Notice that by default, the ResNet18 model’s final layer is defined as the following: \n",
    "\n",
    "`(fc): Linear(in_features=512, out_features=1000, bias=True)`\n",
    "\n",
    "Since the CIFAR-10 dataset has only 10 classes, we need to adjust the final layer so the model produces 10 output scores instead of the default 1000. We can do this by replacing the last layer with a new fully connected layer that maps the same 512 input features to 10 output nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedcb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 10)  # overwrite resnet_model.fc layer to have 10 output scores\n",
    "\n",
    "print(resnet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce984c1",
   "metadata": {},
   "source": [
    "The final layer should now have 10 outputs, matching the exact structure we need to classify CIFAR-10 images!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0c36c",
   "metadata": {},
   "source": [
    "## Train a Model\n",
    "\n",
    "Our ResNet18 model contains over 11 million parameters (weights and biases) that influence how the model classifies images. In order for the model to accurately classify images, it must learn the optimal values for those weights and biases through training. \n",
    "\n",
    "Before we can train our model, we need to define two key components: a loss function and an optimizer.\n",
    "- The [loss function](https://docs.pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#loss-function) measures how far the model’s predictions are from the correct labels. The lower the loss, the better the model is performing.\n",
    "- The [optimizer](https://docs.pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#optimizer) updates the model’s parameters based on the loss, nudging them in directions that reduce future errors. It uses an algorithm called backpropagation to compute how much each parameter contributed to the loss and how it should be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet_model.parameters(), lr=0.001) # lr: learning rate affects how quickly the model adjusts parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059349d",
   "metadata": {},
   "source": [
    "We train the model by iterating over all the images in our training dataset. For each batch of images and their corresponding labels:\n",
    "1. We reset the gradients using `optimizer.zero_grad()` so updates from previous batches don’t accumulate.\n",
    "2. We pass the images through the model to get predictions.\n",
    "3. We calculate the loss between the predictions and true labels.\n",
    "4. We backpropagate the loss using `loss.backward()`, which computes how each parameter should change.\n",
    "5. We update the model parameters with `optimizer.step()`.\n",
    "\n",
    "Over many iterations, the model gradually adjusts its 11 million+ parameters to improve its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83c8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch   500] loss: 1.4802\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for i, (images, labels) in enumerate(train_loader): \n",
    "    optimizer.zero_grad()                 # reset gradients from the previous batch\n",
    "    outputs = resnet_model(images)        # get model predictions for this batch of images\n",
    "    loss = loss_function(outputs, labels) # calculate loss\n",
    "    loss.backward()                       # compute gradients for each weight (how each weight should change to minimize loss)\n",
    "    optimizer.step()                      # update model weights\n",
    "\n",
    "    # Track running loss\n",
    "    running_loss += loss.item()\n",
    "    if (i + 1) % 500 == 0:  # print every 500 mini-batches \n",
    "        print(f\"[Batch {i+1:5d}] loss: {running_loss / 500:.4f}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584de9ce",
   "metadata": {},
   "source": [
    "If you want to learn more about how gradients and backpropagation work, check out the [PyTorch Optimization Tutorial](https://docs.pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296425c1",
   "metadata": {},
   "source": [
    "## Evaluate a Model\n",
    "\n",
    "After training, we need to test how well our model generalizes to unseen data. This step is crucial because a model that performs well on training data might simply be memorizing it rather than learning meaningful patterns — a problem known as overfitting.\n",
    "\n",
    "To evaluate performance, we use the test dataset, which the model has never seen during training. We pass each batch of test images through the model, compare its predictions to the true labels, and compute the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481de0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 62.28%\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "resnet_model.eval()    # set the model to evaluation mode (turns off dropout, etc.)\n",
    "with torch.no_grad():  # disable gradient tracking since we’re not updating weights\n",
    "    for images, labels in test_loader: \n",
    "        outputs = resnet_model(images)                 # get model predictions for a batch of images\n",
    "        _, predicted = torch.max(outputs, 1)           # select the class with the highest score\n",
    "        total += labels.size(0)                        # count total number of images processed\n",
    "        correct += (predicted == labels).sum().item()  # count how many predictions were correct\n",
    "\n",
    "print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41805e82",
   "metadata": {},
   "source": [
    "This gives the model's accuracy percentage or the percentage of test images that were correctly classified. For more advanced analysis of classification models, you could also compute metrics like [precision](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#precision), [recall](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#recall_or_true_positive_rate), or a [confusion matrix](https://www.geeksforgeeks.org/machine-learning/confusion-matrix-machine-learning/), but accuracy is a good starting point for quick model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06b59f",
   "metadata": {},
   "source": [
    "## Save a Model\n",
    "\n",
    "Once training is complete, you can save your model so it can be reused later without retraining. To do so, we will use the `torch.save` method to save the model structure and weights to a file called `model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39179b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet_model, 'model.pth') # save model to model.pth file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43809e",
   "metadata": {},
   "source": [
    "You can then reload the model with all the weights it learned while training using the `torch.load` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = torch.load('model.pth') # load full model stored in model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0ae9d",
   "metadata": {},
   "source": [
    "## Conclusion and Further Readings\n",
    "\n",
    "Congratulations for completing the PyTorch Quick Guide! 🎓🎉 You now have the skills to load, train, and test a PyTorch Model! \n",
    "\n",
    "If you want to learn all the PyTorch basics, check out some of these resources:\n",
    "- [Official PyTorch Begineers Guide](https://docs.pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "- [Deep Learning with PyTorch: A 60 Minute Blitz](https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
