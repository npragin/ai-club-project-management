{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pandas Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Author: Kellen Sullivan\n",
    "\n",
    "This is an introductory tutorial for pandas, a powerful Python library for data handling and analysis. Pandas is widely used to load, explore, and transform datasets, making it an essential tool for anyone working in machine learning or AI. By the end of this tutorial, you will have the data skills necessary to start building your first machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To get started using pandas, you first have to install it! To do so, open a new terminal and run the following command:\n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "You can confirm pandas was successfully installed by importing the package and printing its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "We use `as pd` to import pandas with the alias `pd`. This saves us time from typing out `pandas` everytime we want to refer to the package and instead we can type `pd`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## DataFrames and Series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "DataFrames are the essential underlying datastructure for storing data with pandas. They are tables, with labeled columns and indexed rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Color\": [\"red\", \"blue\", \"green\", \"purple\", \"white\", \"orange\"],\n",
    "        \"Price\" : [5, 8, 3, 4, 9, 5]\n",
    "    }\n",
    ")\n",
    "\n",
    "# display the type of a Pandas DataFrame\n",
    "print(type(df))\n",
    "\n",
    "# display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The DataFrame `df` has two labeled columns, \"Color\" and \"Price\", and an index column with the values 0-5 for each row in the DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Series are a similar datatype in pandas, but contain only one dimension. Unlike DataFrames, Series do not have labeled columns, and instead only have one label for the name of the Series itself. However, Both DataFrames and Series have indexed rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(\n",
    "    [\"red\", \"blue\", \"green\", \"purple\", \"white\"]\n",
    ")\n",
    "\n",
    "# display the type of a Pandas Series\n",
    "print(type(s))\n",
    "\n",
    "# display the Series\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "You can think of a Series as a single column in a DataFrame. Or, using a Python metaphor: if a Series is like a list, then a DataFrame is like a two-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "As displayed above, DataFrames can be created from python dictionaries. However, when working with real-world data you will often want to work with data that is stored in another location. Fortunately, pandas provides many built in functions to easily load data from a variety of sources. \n",
    "\n",
    "For this tutorial, we will explore a popular real-world dataset within the machine learning community about the passengers on the Titanic. The data is stored in a CSV  (Comma-Separated Values) file, which pandas is well suited to handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Use `read_csv` to read in data from a csv file and store it into a Pandas DataFrame. The `read_csv` function takes in the filepath to the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Previewing Data\n",
    "\n",
    "Now that you have loaded in a new data set into a DataFrame, it is time to explore it! Pandas provides two useful functions `head` and `tail` to quickly view a few rows of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Use `head` to display the first 5 rows of a DataFrame. You can also provide a value n to display the first n rows. For example `head(20)` displays the first 20 rows of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default displays the first 5 rows of a DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The tail function works similarly to the head function, but displays the last 5 rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "You may have noticed that the DataFrame has an unlabled index column that was automatically created when we read in the CSV file. In pandas you can set the index column to whatever values you like, including numbers, dates, and even strings! In this case, the column PassengerId appears to be an index column already included in the data. We can use PassengerId as the index by passing in the argument `index_col=\"PassengerId\"` to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\", index_col=\"PassengerId\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### DataFrame Attributes \n",
    "\n",
    "DataFrames have many attributes that allow datascientist to get a quick overview of a dataset. To invoke an attribute use the syntax `dfname.attribute`\n",
    "A few attributes that can quickly provide general information about a data set include\n",
    "- columns\n",
    "- dtypes\n",
    "- shape\n",
    "- size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "`columns` returns a list of all columns in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "`dtypes` displays all columns in a DataFrame and their corresponding data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "`shape` returns a tuple with the number of rows followed by the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "`size` returns the total number of elements in a DataFrame. In other words, it returns the product of the number of rows and columns in a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "If you are curious about what other attributes DataFrames have, check out the official pandas documentation including all DataFrame attributes here: https://pandas.pydata.org/docs/reference/frame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Selecting Data\n",
    "\n",
    "Pandas provides multiple ways to select subsets of data within a DataFrame. In this tutorial we will go over the following methods:\n",
    "\n",
    "- bracket and dot notation\n",
    "- loc and iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### bracket and dot notation\n",
    "\n",
    "The simplest way to select a subset of data is similar to selecting an element in a standard python list or dictionary.\n",
    "\n",
    "- To select a column, use `[]` with the column name inside.\n",
    "- You can also use a `.` followed by the column name. \n",
    "\n",
    "Each syntax will produce the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "When using bracket notation, you can pass in a list to select multiple columns. This is one advantage of using bracket notation. However, both are valid and the best one to use depends on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\"Sex\", \"Age\", \"Survived\"]\n",
    "\n",
    "df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### loc and iloc\n",
    "\n",
    "Pandas provides `loc` and `iloc` as indexing options for more advanced operations. `loc` and `iloc` are invoked similarly, with the row selector first followed by the column selector\n",
    "\n",
    "`loc`  syntax: `df.loc[row_labels, column_labels]`  \n",
    "`iloc` syntax: `df.iloc[row_positions, column_positions]`\n",
    "\n",
    "You can also provide only one selector to `loc` or `iloc`. Pandas will assume you are only selecting based on row and all columns will be included. For example:\n",
    "\n",
    "`df.loc[row_labels]` is also valid syntax\n",
    "\n",
    "The main distinction between `loc` and `iloc` is that `loc` uses label-based selection and `iloc` uses position-based selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "To select an element from a DataFrame using `loc`, provide the row label and column labels to select from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the row with PassengerId of 3\n",
    "df.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the row with PassengerId of 3, and the columns Name and Sex\n",
    "df.loc[3, [\"Name\", \"Sex\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the rows with PassengerId's 3, 4, and 5 and the Age column\n",
    "df.loc[[3,4,5], \"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "To select the same elements using `iloc` we use the index instead of labels. \n",
    "\n",
    "Note that `iloc` uses 0-based indexing, so to select the third row in the DataFrame, which is the row with PassengerId of 3, we use 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the third row in the DataFrame\n",
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the third row, and the third and fourth columns in the DataFrame\n",
    "df.iloc[2, [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the third, fourth, and fifth rows and the fifth column in the DataFrame\n",
    "df.iloc[[2,3,4], 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "You can use the slicing operator `:` to select many rows or columns without listing all of them individually. The following statement selects rows with PassengerId's from 3 to 10, and all columns up until Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3:10, :\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Using `iloc`, a similar subset of the DataFrame can be selected. However, due to `iloc's` 0-based indexing, the row with PassengerId of 3 is not included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3:10, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "One advantage of using `loc` is that it can handle conditional selection. To select based on a conditional statement use the syntax:\n",
    "\n",
    "`df.loc[conditional statement]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all rows with Age < 18\n",
    "df.loc[df[\"Age\"] < 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "Often `loc` and `iloc` are both viable solutions, so deciding when to use which one can be tricky. In general, if the DataFrame has clear row and column lables, or you are using conditional selection, then `loc` is the better option. If instead label names are not relevant, or you are iterating over a DataFrame, then `iloc` may provide a better solution.\n",
    "\n",
    "If you want more practice with `loc` and `iloc`, this article provides many examples, and details what situations favor each selector: https://www.datacamp.com/tutorial/loc-vs-iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### View vs Copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "One important consideration when selecting data in a DataFrame is whether you are working with a view or a copy.\n",
    "- A view references the original data, so any edits you make to the view may also affect the original DataFrame.\n",
    "- A copy, on the other hand, creates a completely new object stored separately in memory. Changes made to a copy will not affect the original data.\n",
    "\n",
    "Depending on how you select data a view or copy will be created. For example, selecting data with a slice creates a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data using a slice creates a view\n",
    "df.loc[:, :\"Age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Selecting data using a conditional value or an iterable list creates a copy instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data using conditional value\n",
    "df.loc[df[\"Age\"] < 18]\n",
    "\n",
    "# select data with an iterable list\n",
    "df.loc[:, [\"Age\", \"Sex\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "In general, views are usually preferred because they provide better performance and use less memory than copies. Check out this resource if you would like to learn more: http://itnext.io/a-guide-to-efficient-data-selection-in-pandas-ea6dab640604"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Exercise: Perform Exploratory Data Analysis\n",
    "\n",
    "**Time to put your knowledge to the test!**\n",
    "\n",
    "An important first step in any machine learning project is to explore the data for intial insights. Pandas provides various functions for machine learning engineers to complete this task.\n",
    "\n",
    "For this exercise you will explore a very popular dataset about passengers on the Titanic. In order to complete this exercise you must complete the following:\n",
    "1. Display the first 10 rows of the DataFrame\n",
    "2. Determine the amount of rows and columns in the DataFrame\n",
    "3. Display all columns and their datatypes\n",
    "4. Create a subset of the original DataFrame that contains all passengers that are female OR under 18 years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Display the first 10 rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine the amount of rows and columns in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Display all columns and their datatypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a DataFrame containing only passengers that are female OR under 18 years (or both)\n",
    "sub_df = ____\n",
    "\n",
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### **Solution**\n",
    "\n",
    "(Try to solve it yourself first before looking!)\n",
    "\n",
    "Click the cells below to reveal a possible solution approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Display the first 10 rows of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine the amount of rows and columns in the DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Display all columns and their datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a DataFrame containing only passengers that are female OR under 18 years old\n",
    "sub_df = df.loc[(df[\"Sex\"] == \"female\") | (df[\"Age\"] < 18)]\n",
    "\n",
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Aggregation \n",
    "\n",
    "Pandas provides many functions to quickly summarize or aggregate data within DataFrames. In this section we will discuss the following:\n",
    "- Aggregate Operations\n",
    "- groupby\n",
    "- value counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Aggregate Operations\n",
    "Pandas provides many aggregate operations to quickly summarize data into a single value. Aggregate operations iterate over a column and return a single value. The following syntax is used to use one:\n",
    "`df['column_name'].aggregate_operation()`\n",
    "\n",
    "If an aggregate operation is used on a DataFrame with more than one column, Pandas will apply the operation to each column in the DataFrame and return a Series containing the result of applying the operation to each column in the DataFrame.\n",
    "\n",
    "Some of the most useful operations include:\n",
    "- mean\n",
    "- mode\n",
    "- sum\n",
    "- count\n",
    "- unique\n",
    "- max\n",
    "- min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the average age of a passenger\n",
    "df[\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a Series containing the mode(s) for a column\n",
    "# The Series will contain multiple modes if there is a tie for the most frequent value\n",
    "df[\"Sex\"].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access the literal mode value, select the first value in the Series using [0]\n",
    "df[\"Sex\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the sum of adding each row in a column\n",
    "df[\"Survived\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of non-null Name's in the DataFrame\n",
    "df[\"Name\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all unqiue values the Embarked column contains\n",
    "df[\"Embarked\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the greatest value in the Fare column\n",
    "df[\"Fare\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the least value in the Fare column\n",
    "df[\"Fare\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### groupby\n",
    "\n",
    "Pandas provides the `groupby` function to split a DataFrame into distinct groups based on a key (usually an existing column in the DataFrame). You can then apply a function to each group individually. The resulting groups are then combined back into one DataFrame. This process is known as the split -> apply -> combine pattern that is commonly used in data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "For example, the following code splits the DataFrame based on the column 'Sex'. It then applys the `sum` function to the male group and female group individually. The results are then combined and displayed in a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Sex\")[\"Survived\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### Value Counts\n",
    "\n",
    "`value_counts` is another useful function for displaying the frequency of unqiue values in a DataFrame. For example, to see how many of the passengers are male and how many are female we can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "The `value_counts` function is often used in combination with the `groupby` function to display more complex relationships. In this situation, `value_counts` acts as the function in the apply step in the split -> apply -> combine patter, and is applied to each group individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Sex\")[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Exercise: Utilize Aggregations\n",
    "\n",
    "**Put your aggregation knowledge to the test!**\n",
    "\n",
    "Now that you have already intially explored the dataset, it is time to use aggregations to uncover deeper insights and better understand the data. In order to complete this exercise you must complete the following:\n",
    "\n",
    "1. Determine the age of the youngest and oldest passengers to survive\n",
    "2. Determine the average fare price for each passenger class \n",
    "3. Determine the number of passengers in each passenger class \n",
    "4. Determine the number of survivors and non-survivors for each passenger class\n",
    "\n",
    "hint: passenger class is denoted by the column Pclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine the age of the youngest and oldest passengers to survive\n",
    "youngest_survivor = ____\n",
    "print(f\"Age of the youngest passenger to survive: {youngest_survivor}\")\n",
    "\n",
    "oldest_survivor = ____\n",
    "print(f\"Age of the oldest passenger to survive: {oldest_survivor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine the average fare price for each passenger class\n",
    "avg_fare_price = ____\n",
    "print(f\"Average fare price for each passenger class: \\n {avg_fare_price}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Determine the number of passengers in each passenger class\n",
    "passenger_count_by_pclass = ____\n",
    "print(f\"Number of passengers by each passenger class: \\n {passenger_count_by_pclass}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Determine the number of survivors and non-survivors for each passenger class\n",
    "survivors_by_pclass = ____\n",
    "print(f\"Survivors by passenger class: \\n {survivors_by_pclass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### **Solution**\n",
    "\n",
    "(Try to solve it yourself first before looking!)\n",
    "\n",
    "Click the cells below to reveal a possible solution approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine the age of the youngest and oldest passengers to survive\n",
    "youngest_survivor = df.loc[df[\"Survived\"] == 1][\"Age\"].min() \n",
    "print(f\"Age of the youngest passenger to survive: {youngest_survivor}\")\n",
    "\n",
    "oldest_survivor = df.loc[df['Survived'] == 1][\"Age\"].max() \n",
    "print(f\"Age of the oldest passenger to survive: {oldest_survivor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine the average fare price for each passenger class\n",
    "avg_fare_price = df.groupby('Pclass')['Fare'].mean()\n",
    "print(f\"Average fare price for each passenger class: \\n {avg_fare_price}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Determine the number of passengers in each passenger class\n",
    "passenger_count_by_pclass = df['Pclass'].value_counts()\n",
    "print(f\"Number of passengers by each passenger class: \\n {passenger_count_by_pclass}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Determine the number of survivors and non-survivors for each passenger class\n",
    "survivors_by_pclass = df.groupby('Pclass')['Survived'].value_counts()\n",
    "print(f\"Survivors by passenger class: \\n {survivors_by_pclass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Transforming Data\n",
    "\n",
    "In machine learning, the columns of a dataset may be referred to as features. Feature engineering is the process of cleaning, transforming, and creating features from the raw data to improve model performance. In practice, machine learning engineers spend 50% to 80% of their time on feature engineering, making it one of the most important skills to master.\n",
    "\n",
    "Thankfully, Pandas provides many methods that make feature engineering simple. In this section, we will review how to do the following: \n",
    "\n",
    "- cast columns to different types\n",
    "- handle null values\n",
    "- create new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Type Casting\n",
    "To change the data type of a column use `astype`.\n",
    "Common dtypes include \n",
    "- Int64\n",
    "- Float64\n",
    "- object\n",
    "- string\n",
    "- datetime64\n",
    "- boolean  \n",
    "\n",
    "Although there are many more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the name column from object to string\n",
    "df[\"Name\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Note that this does not change the original DataFrame, but instead creates a copy of the DataFrame with the updated column type. In order to change the original DataFrame set the original DataFrame equal to the copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the type of the column Name in the original DataFrame\n",
    "df[\"Name\"] = df[\"Name\"].astype(\"String\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "### Address Null Values\n",
    "\n",
    "Most machine learning algorithms cannot directly work with null values, so it is important to have a strategy to handle them. \n",
    "\n",
    "If you are familiar with Python, you may know that null values are represented as `none`. In Pandas, `none` can be used to represent missing values, but there are also other representations. `NaN` (Not a number) indicates a missing numerical value. `NaT` (Not a time) is used to represent missing datime values. And finally `pd.NA` is used as a missing value indicator that applies to all data types. \n",
    "\n",
    "In order to identify null values in a DataFrame, use `isna`. This pandas function returns a Series or DataFrame with True values for every entry that is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "To see how many null values each column has, you can apply the aggreagation operation `sum`. Since True is treated as a 1, and False as a 0, `df.isna().sum()` will return the number of null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "The simplest strategy to handle null values is to remove or \"drop\" them. To drop all rows containing a null value use `dropna`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = df.dropna()\n",
    "\n",
    "dropped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "Note that `df.dropna()` does NOT change the original DataFrame but instead returns a new object with all rows containing null values removed.\n",
    "To update the original DataFrame to not include any null values use `df = df.dropna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "Dropping all rows with a null value can potentially exclude large amounts of valuable data. In this example, `dropna` reduced the number of rows in the DataFrame from 891 to 183!\n",
    "\n",
    "Because of this, machine learning engineers often prefer to \"impute\" or fill in missing values with some other value. One easy way to do this in pandas is to use `fillna` and provide the value to replace null values with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all null values in the age column with 0\n",
    "df[\"Age\"] = df[\"Age\"].fillna(0)\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "One popular imputation strategy when dealing with null values in a machine learning context is mean or mode imputation. \n",
    "\n",
    "If a column contains numerical data, we calculate the mean of the column and then replace all missing values in that column with the mean. A column is numerical, if it has a numeric data type such as integer or float. \n",
    "\n",
    "In machine learning, however, you will often deal with data that is not numeric. For example, the Embarked column takes on the values 'S', 'C', and 'Q'. Since we can't take the average value of this column, we use mode impuation. As a review, the mode of the column is the value that appears most frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null age values with the average age\n",
    "mean_age = df[\"Age\"].mean()\n",
    "df[\"Age\"] = df[\"Age\"].fillna(mean_age)\n",
    "\n",
    "# fill null embarked values with the mode\n",
    "embarked_mode = df[\"Embarked\"].mode()[0]\n",
    "df[\"Embarked\"] = df[\"Embarked\"].fillna(embarked_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "### Create New Columns\n",
    "\n",
    "An important step in the feature engineering process is creating new columns or features. Well-designed features can expose patterns and relationships not immediately present in the raw data, often leading to significant improvements in model performance. \n",
    "\n",
    "Pandas provides several convenient ways to create new columns in a DataFrame.\n",
    "\n",
    "To create a new column with a constant value use `df['new_column_name'] = val`. For example, the following code creates a new column 'Ship' in the DataFrame with the constant value 'Titanic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ship\"] = \"Titanic\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "It is also possible to create new columns based on existing columns in the DataFrame. The following code creates a new column where each row has a new value indicating the age distance from the mean age of everyone on board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age_Centered\"] = df[\"Age\"] - df[\"Age\"].mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "Pandas also allows you to set new columns equal to a condition evaluated based on other columns. In this case, the value in the column 'Is_Female_Survivor' is true when the passenger survived AND they are a female. In this pandas expression we represent and using a single `&` symbol. Pandas uses the following operators for conditional statements.\n",
    "\n",
    "- `&`  - and\n",
    "- `|`  - or\n",
    "- `==` - equal\n",
    "- `!=` - notequal\n",
    "- `>`  - greater than\n",
    "- `<`  - less than\n",
    "- `>=` - greater than or equal to\n",
    "- `<=` - less than or equal to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Is_Female_Survivor\"] = (df[\"Survived\"] == 1) & (df[\"Sex\"] == \"female\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "The `apply` function in pandas lets you apply a custom function to given rows or columns in a DataFrame. Itâ€™s commonly used to create new columns based on the values of existing ones by applying the custom function to each row or column. To use `apply` use the following syntax:\n",
    "\n",
    "`df.apply(function)`, where function is the function you wish to apply.\n",
    "\n",
    "For example, the following code creates a new column 'Is_Child' that gets the value true when age is less than 18, and False otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined function to check if the passenger is a child\n",
    "def is_child(Age):\n",
    "    return Age < 18\n",
    "\n",
    "# Pass the user defined function name as the argument to apply\n",
    "df[\"Is_Child\"] = df[\"Age\"].apply(is_child)\n",
    "df.iloc[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function as the argument to apply\n",
    "df[\"Is_Child\"] = df[\"Age\"].apply(lambda x: True if x < 18 else False)\n",
    "df.iloc[6:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "### Exercise: Feature Engineering\n",
    "\n",
    "**Put your feature engineering skills to the test!**\n",
    "\n",
    "Now that you have already explored the titanic dataset, it is time to perform feature engineering!\n",
    "\n",
    "For this exercise you will alter and add to the existing Titanic DataFrame. In order to complete this exercise you must complete the following:\n",
    "1. Cast the Name column type to 'string'\n",
    "2. Replace all null values in the 'Age' column with the mean age\n",
    "3. Create a new column called 'Fare_centered' that is the difference of the Fare price for a passenger and the mean Fare price.\n",
    "4. Create a new column called 'Adult_male' that is true when a passenger is a male and 18 years or older, and false otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. cast column type here\n",
    "df[\"Name\"] = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Replace all null values in the 'Age' column with the mean age\n",
    "mean_age = ____\n",
    "df[\"Age\"].____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a new column called 'Fare_centered' that is the difference of the Fare price for a passenger and the mean Fare price.\n",
    "df[\"Fare_centered\"] = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a new column called 'Adult_male' that is true when a passenger is a male and 18 years or older, and false otherwise\n",
    "df[\"Is_Adult_Male\"] = ____\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "#### **Solution**\n",
    "\n",
    "(Try to solve it yourself first before looking!)\n",
    "\n",
    "Click the cell below to reveal a possible solution approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. cast column type here\n",
    "df[\"Name\"] = df[\"Name\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Replace all null values in the 'Age' column with the mean age\n",
    "mean_age = df[\"Age\"].mean()\n",
    "df[\"Age\"].fillna(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a new column called 'Fare_centered' that is the difference of the Fare price for a passenger and the mean Fare price.\n",
    "df[\"Fare_centered\"] = df[\"Fare\"] - df[\"Fare\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a new column called 'Adult_male' that is true when a passenger is a male and 18 years or older, and false otherwise\n",
    "df[\"Is_Adult_Male\"] = (df[\"Age\"] >= 18) & (df[\"Sex\"] == \"male\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations for completing the Pandas Introductory Tutorial! ðŸŽ“ðŸŽ‰ You now have the skills to load, explore, and transform a dataset into a form thatâ€™s ready for machine learning!\n",
    "\n",
    "If you want to further your Pandas education, check out some of these resources:\n",
    "- Official Pandas Documentation: https://pandas.pydata.org/docs/user_guide/10min.html\n",
    "- Kaggle Pandas Tutorial: https://www.kaggle.com/learn/pandas\n",
    "- Youtube Channel with various tutorials on Pandas and other Data Science topics: https://www.youtube.com/@robmulla\n",
    "\n",
    "If you want to learn about other popular data processing Python packages, check out Polars! Polars is a modern DataFrame library that improves memory and efficieny from Pandas. Find their official documentation here: https://docs.pola.rs/\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
